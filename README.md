# ü§ñ ExamAI: Advanced AI Study Assistant

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://prep-with-ai.streamlit.app/) üëà Check out ExamAI on streamlit here 

ExamAI is an intelligent study application that transforms static course materials into a dynamic and interactive learning experience. This project was built to solve a common, difficult problem: enabling students to efficiently study from complex or poorly formatted academic documents. By leveraging a sophisticated AI pipeline, students can get accurate answers, generate follow-up questions, and test their knowledge with quizzes, all based on their own notes.

The core mission of this app is to provide a powerful tool for students to prepare extensively for their exams, cover their course materials even within a short period, and deeply understand the content through AI-driven interaction.

## Chosen Tech Stack & Architecture

This application is built on a modern, powerful, and carefully selected stack designed for high-performance AI and a great user experience. The final architecture is the result of significant research and debugging to overcome common RAG (Retrieval-Augmented Generation) pitfalls.

*   **Backend & AI Logic:** **Python**
*   **AI Orchestration:** **LangChain** is the primary framework used to structure the entire RAG pipeline, from data ingestion to final answer generation.
*   **Frontend:** **Streamlit** provides the interactive web application interface.
*   **Vector Database:** **Pinecone** is used as a high-performance, cloud-based vector store for efficient semantic search.
*   **Google Gemini:** The Large Language Model (LLM) used for generating answers, follow-up questions, and quizzes.

### Core Technical Decisions Explained

The journey to a working, relevant RAG system required overcoming several major challenges. The solutions chosen represent a robust and advanced approach to document intelligence.

#### 1. The Text Extraction Problem: `PyMuPDFLoader`
Initial development revealed that many academic PDFs, especially those converted from other formats, contain "garbled text" (e.g., missing spaces) when parsed with standard tools. After testing multiple loaders (`Unstructured`, etc.), **`PyMuPDFLoader`** was chosen for its optimal balance of speed, reliability, and lightweight dependencies, providing a clean baseline of text for the rest of the pipeline.

#### 2. The Relevance Problem: A Multi-Layered Retrieval Strategy
Simply searching for text is not enough to get relevant results. This application uses a sophisticated, multi-stage retrieval process to find the most accurate information:

*   **Advanced Embedding Model (`BAAI/bge-large-en-v1.5`):** Standard embedding models failed to understand the nuances of the (sometimes garbled) academic text. We upgraded to a state-of-the-art, open-source model. This required rebuilding the Pinecone index with a larger vector dimension (`1024`), a critical step that dramatically improved the base semantic search quality.

*   **Hypothetical Document Embeddings (HyDE):** To overcome query ambiguity and API costs from previous methods (`MultiQueryRetriever`), we implemented HyDE. This technique uses a fast, external LLM (**Groq Llama3-8b**) to generate a hypothetical, ideal answer to the user's query *first*. The embedding of this rich, hypothetical answer is then used for the semantic search, resulting in far more relevant document retrieval at virtually no cost.

*   **Keyword Search (`BM25Retriever`):** To complement the semantic search, a traditional keyword search is run in parallel. This ensures that documents containing exact, important terms are not missed.

*   **Cross-Encoder Reranking (`ms-marco-MiniLM-L-6-v2`):** After the hybrid search (HyDE + BM25) gathers a broad set of candidate documents, a final, more powerful Cross-Encoder model meticulously re-scores the candidates against the original user query. This crucial final step filters out the noise and delivers the top 5 most semantically relevant chunks to the language model.

This multi-layered approach ensures that the context provided to the final LLM is as accurate and relevant as possible.

## Setup and Installation

Follow these steps to set up and run the project locally.

1.  **Clone the Repository:**
    ```bash
    git clone <your-repository-url>
    cd exam-ai
    ```

2.  **Create a Virtual Environment:**
    ```bash
    # On macOS/Linux
    python3 -m venv venv
    source venv/bin/activate

    # On Windows
    python -m venv venv
    .\venv\Scripts\activate
    ```

3.  **Install Dependencies:**
    Ensure you have the `requirements.txt` file from the project.
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set Up API Keys:**
    The application requires API keys for Google Gemini, Pinecone, and Groq. Store these securely using Streamlit Secrets. Create a file at `.streamlit/secrets.toml` and add your keys:
    ```toml
    GOOGLE_API_KEY = "YOUR_GOOGLE_API_KEY"
    PINECONE_API_KEY = "YOUR_PINECONE_API_KEY"
    PINECONE_INDEX_NAME = "your-pinecone-index-name"
    GROQ_API_KEY = "YOUR_GROQ_API_KEY"
    ```

5.  **Set Up Pinecone Index:**
    Before the first run, you must create an index in Pinecone with the correct dimensions for the embedding model.
    *   Go to your Pinecone dashboard.
    *   Create a new index with the same name as in your secrets file.
    *   **Dimensions:** `1024`
    *   **Metric:** `Cosine`

6.  **Run the Application:**
    ```bash
    streamlit run main.py
    ```
    The application should now be running and accessible in your web browser.

## Example Usage and Output

The user workflow is simple and intuitive:

1.  The user uploads a PDF document using the sidebar.
2.  The application processes and indexes the document, which involves chunking, embedding, and storing in Pinecone.
3.  The user asks a question in the main input area.

**Example Query:**
> "What is the difference between a primary key and a composite key?"

**Expected Output Format:**
The application will display the results in a clean, tabbed interface:

*   **üí° Answer Tab:**
    *   **Main Answer:** A direct, comprehensive answer generated by the Gemini LLM based on the retrieved context.
    *   **Follow-up Questions:** A list of suggested questions to help the user explore the topic further.

*   **üìù Quiz Tab:**
    *   A multiple-choice quiz is generated based on the topic. Each question is presented clearly, with the answer hidden in an expandable section to encourage active recall.

*   **üîç Retrieved Context Tab:**
    *   This tab displays the top 5 document chunks that were found by the advanced retrieval pipeline (HyDE + BM25 + Reranker). This provides transparency and allows the user to verify the source of the AI's answer.

